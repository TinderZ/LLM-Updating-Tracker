import json
import os
import requests
from datetime import datetime, timedelta
from dateutil import parser
import re
import time
from bs4 import BeautifulSoup

# --- é…ç½®åŒº ---
DATA_FILE = os.path.join(os.path.dirname(__file__), 'data.json')

# åªå…³æ³¨é‡è¦å…¬å¸çš„æ¨¡å‹å…³é”®è¯å’ŒAPIæ˜ å°„
MODEL_PATTERNS = {
    "OpenAI": {
        "keywords": ["gpt", "chatgpt", "o1", "o3", "davinci", "curie", "babbage", "ada"],
        "api_endpoints": ["https://api.github.com/repos/openai/openai-python/releases"],
        "official_sites": ["https://openai.com/blog/rss.xml"]
    },
    "Anthropic": {
        "keywords": ["claude", "sonnet", "opus", "haiku"],
        "api_endpoints": ["https://api.github.com/repos/anthropics/anthropic-sdk-python/releases"],
        "official_sites": ["https://www.anthropic.com/news.rss"]
    },
    "Google": {
        "keywords": ["gemini", "palm", "lamda", "pathways"],
        "api_endpoints": ["https://api.github.com/repos/google/generative-ai-python/releases"],
        "official_sites": ["https://blog.google/technology/ai/rss/"]
    },
    "Meta": {
        "keywords": ["llama", "code llama", "purple llama"],
        "api_endpoints": ["https://api.github.com/repos/facebookresearch/llama/releases"],
        "official_sites": ["https://ai.meta.com/blog/rss/"]
    },
    "xAI": {
        "keywords": ["grok"],
        "api_endpoints": [],
        "official_sites": []
    },
    "DeepSeek": {
        "keywords": ["deepseek"],
        "api_endpoints": ["https://api.github.com/repos/deepseek-ai/DeepSeek-LLM/releases"],
        "official_sites": []
    },
    "Alibaba": {
        "keywords": ["qwen", "é€šä¹‰", "tongyi"],
        "api_endpoints": ["https://api.github.com/repos/QwenLM/Qwen/releases"],
        "official_sites": []
    },
    "Moonshot": {
        "keywords": ["kimi", "moonshot"],
        "api_endpoints": [],
        "official_sites": []
    },
    "ByteDance": {
        "keywords": ["doubao", "è±†åŒ…", "bytedance"],
        "api_endpoints": [],
        "official_sites": []
    }
}

class IntelligentModelTracker:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        
    def load_existing_data(self):
        """åŠ è½½ç°æœ‰æ•°æ®"""
        try:
            with open(DATA_FILE, 'r', encoding='utf-8') as f:
                return json.load(f)
        except (FileNotFoundError, json.JSONDecodeError):
            return []
    
    def save_data(self, data):
        """ä¿å­˜æ•°æ®åˆ°æ–‡ä»¶"""
        # æŒ‰æ—¥æœŸæ’åºï¼Œæœ€æ–°çš„åœ¨å‰é¢
        data.sort(key=lambda x: datetime.strptime(x['update_date'], '%Y-%m-%d'), reverse=True)
        
        with open(DATA_FILE, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=4, ensure_ascii=False)
    
    def fetch_github_releases(self, repo_url):
        """ä»GitHub APIè·å–æœ€æ–°å‘å¸ƒä¿¡æ¯"""
        try:
            response = self.session.get(repo_url, timeout=10)
            if response.status_code == 200:
                releases = response.json()
                return releases[:5]  # åªå–æœ€æ–°çš„5ä¸ªå‘å¸ƒ
        except Exception as e:
            print(f"è·å–GitHubå‘å¸ƒä¿¡æ¯å¤±è´¥: {repo_url}, é”™è¯¯: {e}")
        return []
    
    def extract_model_info_from_github(self, release, company):
        """ä»GitHubå‘å¸ƒä¿¡æ¯ä¸­æå–æ¨¡å‹ä¿¡æ¯"""
        try:
            # è§£æå‘å¸ƒæ—¥æœŸ
            published_at = parser.parse(release['published_at']).strftime('%Y-%m-%d')
            
            # æå–æ¨¡å‹åç§°ï¼ˆä»nameæˆ–tag_nameï¼‰
            model_name = release.get('name', release.get('tag_name', 'Unknown'))

            # å¦‚æœæ¨¡å‹åç§°ä»…ä»…æ˜¯ç‰ˆæœ¬å·ï¼Œåˆ™è·³è¿‡
            if re.fullmatch(r'v?(\d+\.)*\d+', model_name):
                return None
            
            # æ¸…ç†æ¨¡å‹åç§°
            cleaned_model_name = re.sub(r'^v?(\d+\.)*\d+', '', model_name).strip('-').strip()
            
            if not cleaned_model_name:
                cleaned_model_name = model_name

            # æå–ç‰¹æ€§æè¿°
            features = release.get('body', '')[:500]  # é™åˆ¶é•¿åº¦

            # æ£€æŸ¥æ ‡é¢˜æˆ–ç‰¹æ€§æè¿°æ˜¯å¦åŒ…å«å…³é”®å­—ï¼Œå¢åŠ ç›¸å…³æ€§
            title_and_features = f"{cleaned_model_name} {features}"
            if not any(keyword.lower() in title_and_features.lower() for keyword in MODEL_PATTERNS[company]['keywords']):
                 return None
            
            return {
                "company": company,
                "model_name": cleaned_model_name,
                "update_date": published_at,
                "blog_url": "",
                "license_type": "unknown",
                "features": features,
            }
        except Exception as e:
            print(f"è§£æGitHubå‘å¸ƒä¿¡æ¯å¤±è´¥: {e}")
        return None
    
    def is_today_update(self, update_date):
        """æ£€æŸ¥æ˜¯å¦ä¸ºä»Šå¤©å‘å¸ƒçš„æ›´æ–°"""
        try:
            update_dt = datetime.strptime(update_date, '%Y-%m-%d')
            today = datetime.now().date()
            return update_dt.date() == today
        except:
            return False
    
    def is_duplicate(self, new_item, existing_data):
        """æ£€æŸ¥æ˜¯å¦ä¸ºé‡å¤æ•°æ®"""
        for item in existing_data:
            # æ£€æŸ¥æ¨¡å‹åç§°å’Œå…¬å¸æ˜¯å¦ç›¸åŒ
            if (item.get('company') == new_item.get('company') and 
                item.get('model_name') == new_item.get('model_name')):
                return True
        
        return False
    
    def fetch_official_rss_updates(self):
        """ä»å®˜æ–¹RSSæºè·å–AIæ¨¡å‹æ›´æ–°ä¿¡æ¯"""
        import feedparser
        
        updates = []
        for company, info in MODEL_PATTERNS.items():
            for rss_url in info.get('official_sites', []):
                try:
                    print(f"  æ­£åœ¨æ£€æŸ¥ {company} å®˜æ–¹RSS...")
                    feed = feedparser.parse(rss_url)
                    
                    for entry in feed.entries[:5]:  # åªå–æœ€æ–°5æ¡
                        title_lower = entry.title.lower()
                        summary_lower = entry.get('summary', '').lower()
                        
                        # æ£€æŸ¥æ˜¯å¦åŒ…å«æ¨¡å‹ç›¸å…³å…³é”®è¯
                        if any(keyword.lower() in title_lower or keyword.lower() in summary_lower 
                               for keyword in info['keywords']):
                            # æ£€æŸ¥æ˜¯å¦ä¸ºæ¨¡å‹å‘å¸ƒç›¸å…³
                            if any(word in title_lower for word in ['release', 'launch', 'announce', 'unveil', 'introduce', 'available', 'new']):
                                published_date = entry.get('published_parsed')
                                if published_date:
                                    update_date = time.strftime('%Y-%m-%d', published_date)
                                else:
                                    update_date = datetime.now().strftime('%Y-%m-%d')
                                
                                # åªè¦ä»Šå¤©å‘å¸ƒçš„
                                if self.is_today_update(update_date):
                                    updates.append({
                                        "company": company,
                                        "model_name": entry.title,
                                        "update_date": update_date,
                                        "blog_url": entry.get('link', ''),
                                        "license_type": "unknown",
                                        "features": entry.get('summary', '')[:300],
                                    })
                                    print(f"    âœ… å‘ç°ä»Šæ—¥å®˜æ–¹å‘å¸ƒ: {entry.title}")
                except Exception as e:
                    print(f"    âŒ è·å– {company} RSSå¤±è´¥: {e}")
                    continue
        
        return updates
    
    def fetch_all_updates(self):
        """è·å–æ‰€æœ‰æ¥æºçš„ä»Šæ—¥æ›´æ–°"""
        today_str = datetime.now().strftime('%Y-%m-%d')
        print(f"ğŸ¤– å¼€å§‹æŠ“å– {today_str} çš„AIæ¨¡å‹æ›´æ–°...")
        print("ğŸ“‹ å…³æ³¨å…¬å¸: OpenAI, Anthropic, Google, Meta, xAI, DeepSeek, Alibaba(Qwen), Moonshot(Kimi), ByteDance(è±†åŒ…)")
        
        existing_data = self.load_existing_data()
        new_updates = []
        
        # 1. ä»å®˜æ–¹RSSæºè·å–æ›´æ–°
        print("\nğŸ“° æ­£åœ¨ä»å®˜æ–¹RSSæºè·å–ä»Šæ—¥æ›´æ–°...")
        rss_updates = self.fetch_official_rss_updates()
        for update in rss_updates:
            if not self.is_duplicate(update, existing_data + new_updates):
                new_updates.append(update)
        
        # 2. ä»GitHubè·å–æ›´æ–°
        print("\nğŸ“¦ æ­£åœ¨ä»GitHubè·å–ä»Šæ—¥æ›´æ–°...")
        for company, info in MODEL_PATTERNS.items():
            for api_endpoint in info['api_endpoints']:
                print(f"  æ­£åœ¨æ£€æŸ¥ {company} GitHub...")
                releases = self.fetch_github_releases(api_endpoint)
                for release in releases:
                    model_info = self.extract_model_info_from_github(release, company)
                    if (model_info and 
                        self.is_today_update(model_info['update_date']) and
                        not self.is_duplicate(model_info, existing_data + new_updates)):
                        new_updates.append(model_info)
                        print(f"    âœ… å‘ç°ä»Šæ—¥GitHubå‘å¸ƒ: {model_info['model_name']}")
        
        # 4. ä¿å­˜æ›´æ–°
        if new_updates:
            print(f"\nğŸ‰ å‘ç° {len(new_updates)} ä¸ªä»Šæ—¥æ–°å‘å¸ƒçš„æ¨¡å‹ï¼Œæ­£åœ¨ä¿å­˜...")
            all_data = existing_data + new_updates
            self.save_data(all_data)
            print("âœ… æ•°æ®å·²æˆåŠŸæ›´æ–°åˆ° data.json")
            
            # æ‰“å°æ–°æ›´æ–°æ‘˜è¦
            print(f"\nğŸ“‹ {today_str} æ–°å‘å¸ƒæ¨¡å‹æ‘˜è¦:")
            for update in new_updates:
                print(f"  â€¢ {update['company']}: {update['model_name']} (æ¥æº: {update['source']})")
        else:
            print(f"â„¹ï¸  {today_str} æš‚æœªå‘ç°æŒ‡å®šå…¬å¸çš„æ–°æ¨¡å‹å‘å¸ƒ")

def fetch_updates():
    """ä¸»å‡½æ•°ï¼šæ™ºèƒ½æŠ“å–AIæ¨¡å‹æ›´æ–°"""
    tracker = IntelligentModelTracker()
    tracker.fetch_all_updates()

if __name__ == "__main__":
    fetch_updates()